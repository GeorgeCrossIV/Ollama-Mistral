{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0cc6c7cb-0245-46a2-aabc-3dd9f1b4b086",
      "metadata": {
        "id": "0cc6c7cb-0245-46a2-aabc-3dd9f1b4b086"
      },
      "source": [
        "# Homework: Implementing a RAG Example with Ollama and Mistral LLM\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this homework, you will be working on a practical application of the DataStax RAGStack. The goal is to modify this Jupyter Notebook that currently leverages OpenAI's LLM (Large Language Models) for a RAG example. Your task is to adapt this notebook to use Ollama running Mistral LLM, as the backbone for the RAG implementation.\n",
        "\n",
        "## Why Ollama?\n",
        "\n",
        "Ollama offers the option to run a LLM on a local machine. Self-managed LLMs are especially of interest for Customers using Cassandra or DSE on-prem and in internet-restricted environments, and for those using Cassandra, DSE and Astra DB who are cautious about sending sensitive data to cloud-based LLM services due to privacy concerns and cost considerations. Ollama enables local execution of Large Language Models, providing a private solution. This is particularly beneficial for demonstrations and aligns with the requirements of businesses handling critical data, ensuring it remains within their controlled environment.\n",
        "\n",
        "For those seeking self-managed LLMs, alternatives like Mistral are available, offering performance comparable to OpenAI's models. Interested parties are encouraged to [review the Mistral documentation](https://huggingface.co/docs/transformers/main/en/model_doc/mistral). Mistral is designed for easy installation and can be efficiently hosted on the robust computing resources available in customer data centers.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. **Understand the Current Implementation**: Begin by familiarizing yourself with the existing Jupyter Notebook. It uses DataStax's RAGStack, integrating Astra DB as a vector store, and employs an OpenAI LLM for generating responses.\n",
        "\n",
        "2. **Transition to Ollama and Mistral LLM**: Your primary task is to modify the code in the notebook to replace the OpenAI LLM with Ollama running Mistral LLM. This will involve understanding the differences between the two models and adapting the API calls and data handling accordingly.\n",
        "\n",
        "3. **Test and Validate**: Keep in mind you run notebook and ollama on your local machine. After implementing the changes, test the notebook to ensure that it functions correctly with the new LLM.\n",
        "\n",
        "## Resources\n",
        "\n",
        "- **Ollama Documentation**: [How to install Ollama](https://ollama.com/download) and [How to run Mistral LLM powered by Ollama](https://ollama.com/library/mistral/tags)\n",
        "\n",
        "## Submission Guidelines\n",
        "\n",
        "- Complete the task in the provided Jupyter Notebook.\n",
        "- Ensure all code cells are well-documented.\n",
        "- Submit the final notebook along with a brief report summarizing your approach, key challenges, and solutions.\n",
        "\n",
        "Good luck, and feel free to reach out if you have any questions or need further clarifications!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e5e9d7-37bd-474b-9c75-f3a82daf41c0",
      "metadata": {
        "id": "d4e5e9d7-37bd-474b-9c75-f3a82daf41c0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%pip install ragstack-ai sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a4f99fe7-000b-4dcb-a89b-b4e784daed0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4f99fe7-000b-4dcb-a89b-b4e784daed0f",
        "outputId": "9fcf72de-b8d1-4c70-a3c5-745861910a3c"
      },
      "outputs": [],
      "source": [
        "import getpass, openai, configparser\n",
        "\n",
        "# Create a config parser object\n",
        "config = configparser.ConfigParser()\n",
        "\n",
        "# Read the config.ini file\n",
        "config.read('config.ini')\n",
        "\n",
        "# Access the configurations\n",
        "ASTRA_DB_API_ENDPOINT = config['Settings']['ASTRA_DB_API_ENDPOINT']\n",
        "ASTRA_DB_APPLICATION_TOKEN = config['Settings']['ASTRA_DB_APPLICATION_TOKEN']\n",
        "OPENAI_API_KEY = config['Settings']['OPENAI_API_KEY']\n",
        "lm_studio_model = config['Settings']['LM_STUDIO_MODEL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b8e3ddd0-5078-4398-8ef2-3465b7770dcd",
      "metadata": {
        "id": "b8e3ddd0-5078-4398-8ef2-3465b7770dcd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "61622a71-de5f-44e7-bc06-ae4555530304",
      "metadata": {
        "id": "61622a71-de5f-44e7-bc06-ae4555530304"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
        "text = response.text\n",
        "\n",
        "f = open('essay.txt', 'w')\n",
        "f.write(text)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "079ea2c2-b256-4942-aec8-cbf6b69d5697",
      "metadata": {
        "id": "079ea2c2-b256-4942-aec8-cbf6b69d5697"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import AstraDB\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ae5374",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip show langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "bb2b6310-7b61-4cd1-b329-b8e52275cca5",
      "metadata": {
        "id": "bb2b6310-7b61-4cd1-b329-b8e52275cca5",
        "outputId": "08265d37-fa3c-4545-c99c-2a55d8f6d8f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Based on the provided context, there are no specific two main things that the author worked on before college. The response is not relevant to the given context.\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "loader = TextLoader(\"essay.txt\")\n",
        "docs = loader.load()\n",
        "# Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "# Define the embedding model\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "#vector = load_vector_store()\n",
        "vector = AstraDB.from_documents(documents, embeddings,collection_name=\"openai_demo\", api_endpoint=ASTRA_DB_API_ENDPOINT, token=ASTRA_DB_APPLICATION_TOKEN)\n",
        "# Define a retriever interface\n",
        "retriever = vector.as_retriever()\n",
        "# Define LLM\n",
        "#model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
        "model = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
        "# Define prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "# Create a retrieval chain to answer questions\n",
        "document_chain = create_stuff_documents_chain(model, prompt)\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "response = retrieval_chain.invoke({\"input\": \"What were the two main things the author worked on before college?\"})\n",
        "print(response[\"answer\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
